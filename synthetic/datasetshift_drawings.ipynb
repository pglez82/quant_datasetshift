{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import *\n",
    "from sklearn.utils import check_array, check_random_state\n",
    "from sklearn.utils import shuffle as util_shuffle\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "import numpy as np\n",
    "\n",
    "def _generate_hypercube(samples, dimensions, rng):\n",
    "    \"\"\"Returns distinct binary samples of length dimensions.\"\"\"\n",
    "    if dimensions > 30:\n",
    "        return np.hstack(\n",
    "            [\n",
    "                rng.randint(2, size=(samples, dimensions - 30)),\n",
    "                _generate_hypercube(samples, 30, rng),\n",
    "            ]\n",
    "        )\n",
    "    out = sample_without_replacement(2**dimensions, samples, random_state=rng).astype(\n",
    "        dtype=\">u4\", copy=False\n",
    "    )\n",
    "    out = np.unpackbits(out.view(\">u1\")).reshape((-1, 32))[:, -dimensions:]\n",
    "    return out\n",
    "\n",
    "def make_classification(\n",
    "    n_samples=100,\n",
    "    n_features=20,\n",
    "    *,\n",
    "    n_informative=2,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=2,\n",
    "    weights=None,\n",
    "    flip_y=0.01,\n",
    "    class_sep=1.0,\n",
    "    hypercube=True,\n",
    "    shift=0.0,\n",
    "    scale=1.0,\n",
    "    shuffle=True,\n",
    "    random_state=None,\n",
    "):\n",
    "    \"\"\"Generate a random n-class classification problem.\n",
    "    This initially creates clusters of points normally distributed (std=1)\n",
    "    about vertices of an ``n_informative``-dimensional hypercube with sides of\n",
    "    length ``2*class_sep`` and assigns an equal number of clusters to each\n",
    "    class. It introduces interdependence between these features and adds\n",
    "    various types of further noise to the data.\n",
    "    Without shuffling, ``X`` horizontally stacks features in the following\n",
    "    order: the primary ``n_informative`` features, followed by ``n_redundant``\n",
    "    linear combinations of the informative features, followed by ``n_repeated``\n",
    "    duplicates, drawn randomly with replacement from the informative and\n",
    "    redundant features. The remaining features are filled with random noise.\n",
    "    Thus, without shuffling, all useful features are contained in the columns\n",
    "    ``X[:, :n_informative + n_redundant + n_repeated]``.\n",
    "    Read more in the :ref:`User Guide <sample_generators>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, default=100\n",
    "        The number of samples.\n",
    "    n_features : int, default=20\n",
    "        The total number of features. These comprise ``n_informative``\n",
    "        informative features, ``n_redundant`` redundant features,\n",
    "        ``n_repeated`` duplicated features and\n",
    "        ``n_features-n_informative-n_redundant-n_repeated`` useless features\n",
    "        drawn at random.\n",
    "    n_informative : int, default=2\n",
    "        The number of informative features. Each class is composed of a number\n",
    "        of gaussian clusters each located around the vertices of a hypercube\n",
    "        in a subspace of dimension ``n_informative``. For each cluster,\n",
    "        informative features are drawn independently from  N(0, 1) and then\n",
    "        randomly linearly combined within each cluster in order to add\n",
    "        covariance. The clusters are then placed on the vertices of the\n",
    "        hypercube.\n",
    "    n_redundant : int, default=2\n",
    "        The number of redundant features. These features are generated as\n",
    "        random linear combinations of the informative features.\n",
    "    n_repeated : int, default=0\n",
    "        The number of duplicated features, drawn randomly from the informative\n",
    "        and the redundant features.\n",
    "    n_classes : int, default=2\n",
    "        The number of classes (or labels) of the classification problem.\n",
    "    n_clusters_per_class : int, default=2\n",
    "        The number of clusters per class.\n",
    "    weights : array-like of shape (n_classes,) or (n_classes - 1,),\\\n",
    "              default=None\n",
    "        The proportions of samples assigned to each class. If None, then\n",
    "        classes are balanced. Note that if ``len(weights) == n_classes - 1``,\n",
    "        then the last class weight is automatically inferred.\n",
    "        More than ``n_samples`` samples may be returned if the sum of\n",
    "        ``weights`` exceeds 1. Note that the actual class proportions will\n",
    "        not exactly match ``weights`` when ``flip_y`` isn't 0.\n",
    "    flip_y : float, default=0.01\n",
    "        The fraction of samples whose class is assigned randomly. Larger\n",
    "        values introduce noise in the labels and make the classification\n",
    "        task harder. Note that the default setting flip_y > 0 might lead\n",
    "        to less than ``n_classes`` in y in some cases.\n",
    "    class_sep : float, default=1.0\n",
    "        The factor multiplying the hypercube size.  Larger values spread\n",
    "        out the clusters/classes and make the classification task easier.\n",
    "    hypercube : bool, default=True\n",
    "        If True, the clusters are put on the vertices of a hypercube. If\n",
    "        False, the clusters are put on the vertices of a random polytope.\n",
    "    shift : float, ndarray of shape (n_features,) or None, default=0.0\n",
    "        Shift features by the specified value. If None, then features\n",
    "        are shifted by a random value drawn in [-class_sep, class_sep].\n",
    "    scale : float, ndarray of shape (n_features,) or None, default=1.0\n",
    "        Multiply features by the specified value. If None, then features\n",
    "        are scaled by a random value drawn in [1, 100]. Note that scaling\n",
    "        happens after shifting.\n",
    "    shuffle : bool, default=True\n",
    "        Shuffle the samples and the features.\n",
    "    random_state : int, RandomState instance or None, default=None\n",
    "        Determines random number generation for dataset creation. Pass an int\n",
    "        for reproducible output across multiple function calls.\n",
    "        See :term:`Glossary <random_state>`.\n",
    "    Returns\n",
    "    -------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        The generated samples.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        The integer labels for class membership of each sample.\n",
    "    See Also\n",
    "    --------\n",
    "    make_blobs : Simplified variant.\n",
    "    make_multilabel_classification : Unrelated generator for multilabel tasks.\n",
    "    Notes\n",
    "    -----\n",
    "    The algorithm is adapted from Guyon [1] and was designed to generate\n",
    "    the \"Madelon\" dataset.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n",
    "           selection benchmark\", 2003.\n",
    "    \"\"\"\n",
    "    generator = check_random_state(random_state)\n",
    "\n",
    "    # Count features, clusters and samples\n",
    "    if n_informative + n_redundant + n_repeated > n_features:\n",
    "        raise ValueError(\n",
    "            \"Number of informative, redundant and repeated \"\n",
    "            \"features must sum to less than the number of total\"\n",
    "            \" features\"\n",
    "        )\n",
    "    # Use log2 to avoid overflow errors\n",
    "    if n_informative < np.log2(n_classes * n_clusters_per_class):\n",
    "        msg = \"n_classes({}) * n_clusters_per_class({}) must be\"\n",
    "        msg += \" smaller or equal 2**n_informative({})={}\"\n",
    "        raise ValueError(\n",
    "            msg.format(\n",
    "                n_classes, n_clusters_per_class, n_informative, 2**n_informative\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if weights is not None:\n",
    "        if len(weights) not in [n_classes, n_classes - 1,n_classes*n_clusters_per_class]:\n",
    "            raise ValueError(\n",
    "                \"Weights specified but incompatible with number of classes.\"\n",
    "            )\n",
    "        if len(weights) == n_classes*n_clusters_per_class:\n",
    "            print(\"Received weights for each of the clusters\")\n",
    "        elif len(weights) == n_classes - 1:\n",
    "            if isinstance(weights, list):\n",
    "                weights = weights + [1.0 - sum(weights)]\n",
    "            else:\n",
    "                weights = np.resize(weights, n_classes)\n",
    "                weights[-1] = 1.0 - sum(weights[:-1])\n",
    "    else:\n",
    "        weights = [1.0 / n_classes] * n_classes\n",
    "\n",
    "    n_useless = n_features - n_informative - n_redundant - n_repeated\n",
    "    n_clusters = n_classes * n_clusters_per_class\n",
    "\n",
    "    # Distribute samples among clusters by weight\n",
    "    if len(weights)==n_classes:\n",
    "        n_samples_per_cluster = [\n",
    "            int(n_samples * weights[k % n_classes] / n_clusters_per_class)\n",
    "            for k in range(n_clusters)\n",
    "        ]\n",
    "    else:\n",
    "        n_samples_per_cluster = [\n",
    "            int(n_samples * weights[k])\n",
    "            for k in range(n_clusters)\n",
    "        ]\n",
    "        print(\"n_samples_per_cluster\",n_samples_per_cluster)\n",
    "\n",
    "    for i in range(n_samples - sum(n_samples_per_cluster)):\n",
    "        n_samples_per_cluster[i % n_clusters] += 1\n",
    "\n",
    "    # Initialize X and y\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    y = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    # Build the polytope whose vertices become cluster centroids\n",
    "    centroids = _generate_hypercube(n_clusters, n_informative, generator).astype(\n",
    "        float, copy=False\n",
    "    )\n",
    "    centroids *= 2 * class_sep\n",
    "    centroids -= class_sep\n",
    "    if not hypercube:\n",
    "        centroids *= generator.uniform(size=(n_clusters, 1))\n",
    "        centroids *= generator.uniform(size=(1, n_informative))\n",
    "\n",
    "    # Initially draw informative features from the standard normal\n",
    "    X[:, :n_informative] = generator.standard_normal(size=(n_samples, n_informative))\n",
    "\n",
    "    # Create each cluster; a variant of make_blobs\n",
    "    stop = 0\n",
    "    for k, centroid in enumerate(centroids):\n",
    "        start, stop = stop, stop + n_samples_per_cluster[k]\n",
    "        y[start:stop] = k % n_classes  # assign labels\n",
    "        X_k = X[start:stop, :n_informative]  # slice a view of the cluster\n",
    "\n",
    "        A = 2 * generator.uniform(size=(n_informative, n_informative)) - 1\n",
    "        X_k[...] = np.dot(X_k, A)  # introduce random covariance\n",
    "\n",
    "        X_k += centroid  # shift the cluster to a vertex\n",
    "\n",
    "    # Create redundant features\n",
    "    if n_redundant > 0:\n",
    "        B = 2 * generator.uniform(size=(n_informative, n_redundant)) - 1\n",
    "        X[:, n_informative : n_informative + n_redundant] = np.dot(\n",
    "            X[:, :n_informative], B\n",
    "        )\n",
    "\n",
    "    # Repeat some features\n",
    "    if n_repeated > 0:\n",
    "        n = n_informative + n_redundant\n",
    "        indices = ((n - 1) * generator.uniform(size=n_repeated) + 0.5).astype(np.intp)\n",
    "        X[:, n : n + n_repeated] = X[:, indices]\n",
    "\n",
    "    # Fill useless features\n",
    "    if n_useless > 0:\n",
    "        X[:, -n_useless:] = generator.standard_normal(size=(n_samples, n_useless))\n",
    "\n",
    "    # Randomly replace labels\n",
    "    if flip_y >= 0.0:\n",
    "        flip_mask = generator.uniform(size=n_samples) < flip_y\n",
    "        y[flip_mask] = generator.randint(n_classes, size=flip_mask.sum())\n",
    "\n",
    "    # Randomly shift and scale\n",
    "    if shift is None:\n",
    "        shift = (2 * generator.uniform(size=n_features) - 1) * class_sep\n",
    "    X += shift\n",
    "\n",
    "    if scale is None:\n",
    "        scale = 1 + 100 * generator.uniform(size=n_features)\n",
    "    X *= scale\n",
    "\n",
    "    if shuffle:\n",
    "        # Randomly permute samples\n",
    "        X, y = util_shuffle(X, y, random_state=generator)\n",
    "\n",
    "        # Randomly permute features\n",
    "        indices = np.arange(n_features)\n",
    "        generator.shuffle(indices)\n",
    "        X[:, :] = X[:, indices]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "colorbar() missing 1 required positional argument: 'mappable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/media/nas/pgonzalez/quant_datasetshift/synthetic/datasetshift_drawings.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.35.105.27/media/nas/pgonzalez/quant_datasetshift/synthetic/datasetshift_drawings.ipynb#ch0000000vscode-remote?line=30'>31</a>\u001b[0m f, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.35.105.27/media/nas/pgonzalez/quant_datasetshift/synthetic/datasetshift_drawings.ipynb#ch0000000vscode-remote?line=31'>32</a>\u001b[0m f\u001b[39m.\u001b[39mtight_layout()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B156.35.105.27/media/nas/pgonzalez/quant_datasetshift/synthetic/datasetshift_drawings.ipynb#ch0000000vscode-remote?line=32'>33</a>\u001b[0m ax_c \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mcolorbar()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.35.105.27/media/nas/pgonzalez/quant_datasetshift/synthetic/datasetshift_drawings.ipynb#ch0000000vscode-remote?line=33'>34</a>\u001b[0m ax_c\u001b[39m.\u001b[39mset_label(\u001b[39m\"\u001b[39m\u001b[39m$P(y = 1)$\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B156.35.105.27/media/nas/pgonzalez/quant_datasetshift/synthetic/datasetshift_drawings.ipynb#ch0000000vscode-remote?line=34'>35</a>\u001b[0m ax_c\u001b[39m.\u001b[39mset_ticks([\u001b[39m0\u001b[39m, \u001b[39m.25\u001b[39m, \u001b[39m.5\u001b[39m, \u001b[39m.75\u001b[39m, \u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: colorbar() missing 1 required positional argument: 'mappable'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "seed = 2040\n",
    "\n",
    "\n",
    "def create_dataset(n_clusters_per_class, weights=None, seed=2032):\n",
    "    X, y = make_classification(n_samples=1000,n_features=2, n_redundant=0, n_informative=2, weights=weights,n_clusters_per_class=n_clusters_per_class,random_state=seed,class_sep=2)\n",
    "    return pd.DataFrame(data={\"x1\":X[:,0],\"x2\":X[:,1],\"y\":y})\n",
    "\n",
    "\n",
    "def plot_logisticregression(clf,dataset,f,ax,plotx):\n",
    "    ax = ax[plotx]\n",
    "    xx, yy = np.mgrid[-5:5:.01, -5:5:.01]\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    probs = clf.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n",
    "                        vmin=0, vmax=1)\n",
    "\n",
    "    if plotx==3:\n",
    "        ax_c = f.colorbar()\n",
    "        ax_c.set_label(\"$P(y = 1)$\")\n",
    "        ax_c.set_ticks([0, .25, .5, .75, 1])\n",
    "\n",
    "    negatives = dataset[dataset.y==0]\n",
    "    positives = dataset[dataset.y==1]\n",
    "\n",
    "    ax.scatter(negatives[['x1']], negatives[['x2']], s=50,edgecolor=\"white\", linewidth=1, color='red')\n",
    "    ax.scatter(positives[['x1']], positives[['x2']], s=50,edgecolor=\"white\", linewidth=1, color='blue')\n",
    "\n",
    "    ax.set(aspect=\"equal\",xlim=(-5, 5), ylim=(-5, 5),xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(1,3,figsize=(10, 8))\n",
    "f.tight_layout()\n",
    "\n",
    "\n",
    "dataset = create_dataset(2,weights=[0.25,0.25,0.25,0.25],seed=seed)\n",
    "clf = LogisticRegression().fit(dataset[['x1', 'x2']], dataset['y'])\n",
    "plot_logisticregression(clf,dataset,f,ax,0)\n",
    "\n",
    "dataset = create_dataset(2,weights=[0.05,0.05,0.45,0.45],seed=seed)\n",
    "clf = LogisticRegression().fit(dataset[['x1', 'x2']], dataset['y'])\n",
    "plot_logisticregression(clf,dataset,f,ax,1)\n",
    "\n",
    "dataset = create_dataset(2,weights=[0.02,0.48,0.48,0.02],seed=seed)\n",
    "clf = LogisticRegression().fit(dataset[['x1', 'x2']], dataset['y'])\n",
    "plot_logisticregression(clf,dataset,f,ax,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aebf59cd4c26cc229022e8f1603fcc12869c5f6ee7fb2cbcb8abbf56aa0602b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
